\documentclass[english]{scrartcl}
\usepackage[osf]{mathpazo}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage[usenames,dvipsnames]{color}
\usepackage{tikz}
\usepackage{gnuplot-lua-tikz}
\usepackage{amsmath,amssymb}
\usepackage{capt-of}
\usepackage{listings}
\usepackage{booktabs}
\lstset{language=xml,numbers=left,stepnumber=5,numberstyle=\ttfamily\color{Purple},basicstyle=\small\ttfamily,keywordstyle=\itshape,stringstyle=\color{Bittersweet},morekeywords={Expression,Function,Parameter,VectorField,StateVariable}}

\setcapindent{0em}
\addtokomafont{captionlabel}{\bfseries}
\addtokomafont{title}{\rmfamily}

\author{Andrei Kramer}
\title{Analysis of a large Sample}
\subject{MCMC\_CLIB}

\begin{document}
\maketitle
\begin{abstract}
  This Supplement provides some illustration for the chosen model. It
  shows, how a parameter sample is related to a model fit. This model
  has been chosen as an example for a hard problem: the model is large
  and cannot fit all of the observations simultaneously; the
  measurements are sparse, uncertain, and are provided in arbitrary
  units. Note: The reason for the missmatch will not be discussed. The
  data might have been obtained under erroneous assumptions which are
  consequently not reflected in the modeling setup or alternatively,
  the model is incorrect.
\end{abstract}
\section{Two different ways to sample the provided model}

\begin{figure}
  \hspace*{-1cm}
  \input{sample}
  \caption{The Posterior Sample of size $N=1\times10^6$, as obtained
    from the \texttt{MCMC\_CLIB}; thinned out using the integrated
    autocorrelation length $\tau_{\text{int.},l}=1520(250)$ (estimated
    using the log-posterior $l$ as observable). The red line indicates
    the maximum posterior estimate, while the green errorbars indicate
    the used prior distribution (Gaussian). The colorpalette reflects
    the value of the log-posterior for each sampled point.  Here, we
    have incorporated our prior knowledge about the parameter values
    in the prior parameter density. Yet, we have chosen to let the
    prior span several orders of magnitude.\label{fig:pc-sample}}
\end{figure}

The sample was obtained in $t_{\text{s}}=267385$\,s (about
$74$\,h). Considering the auto-correlation
$\tau_{\text{int.},l}=1520(250)$, the effective sampling speed is:
\begin{equation}
  \label{eq:speed}
  v=\frac{N}{2\tau_{\text{int.},l} t_{\text{s}}}=12(2)\times 10^{-4}\,\text{s}^{-1}\,.
\end{equation}
The properties recorded during sampling are summarised in
Table~\ref{tab:sp}. The sample is plotted in
Figure~\ref{fig:pc-sample} using parallel coordinates (each sampled
parameter vector is shown component wise, as a
line). Figure~\ref{fig:osmpl} shows the output trajectories obtained
from forward simulations using the sampled parameters. There is
partial agreement between model and data (black errorbars).
\begin{figure}
  \sffamily\tiny
  \hspace*{-3cm}
  \begin{tikzpicture}
    \node (plot) {\input{SampleOutputsTauUpper}};
    \node (Ytitle) at (plot.north) {model outputs};
    \foreach \i in {1,...,6} \node (Ylabel\i) at (-9.25+2.75*\i,7) {$y_\i$};
    \node (Tlabel) at (plot.south) {time $t$ in h};
    \node (Tdescription) [above of=Tlabel] {measurements were taken at $\{0.1,\,8,\,24,\,48,\,72\}$ h};
    \node (Utitle) at (-9,0) [rotate=90]{experiment conditions (inputs)};
    \foreach \i in {1,...,4} \node (Ulabel\i) at (-8.5,8.4-3.3*\i) {$u_\i$};
  \end{tikzpicture}
  \caption{output trajectories per parameter sample member;
    to reduce the number of lines these trajectory lines are once
    again plotted for every 3534th
    ($2(\tau_{\text{int.},l}+\delta_\tau)$) parameter in the parameter
    sample. The green, vertical errorbars represent the data and its
    measurement error estimation. The data is sparse (not all outputs
    were observed at all measurement instances. The model does not fit
    all data points.)\label{fig:osmpl}}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{rl}
    \toprule
    tuning duration& $1000$ points\\
    target acceptance $a_0$&$0.50$\\
    observed acceptance $a$&$0.27$\\
    auto-correlation length $\tau_{\text{int.},l}$&$1520(250)$\\
    failed Likelihood evaluations \texttt{[CVODES]}&$16$\\
    \bottomrule
  \end{tabular}
  %\captionof{table}{Sample Properties.\label{tab:sp}}
  \caption{Sample Properties.\label{tab:sp}}
\end{table}

\begin{figure}
  \centering
  \hspace*{-3cm}
  \sffamily\tiny
  \begin{tikzpicture}
    \node (plot) {\input{SampleOutputs}};
    \node (Ytitle) at (plot.north) {model outputs};
    \foreach \i in {1,...,6} \node (Ylabel\i) at (-9.25+2.75*\i,7) {$y_\i$};
    \node (Tlabel) at (plot.south) {time $t$ in h};
    \node (Tdescription) [above of=Tlabel] {measurements were taken at $\{0.1,\,8,\,24,\,48,\,72\}$ h};
    \node (Utitle) at (-9,0) [rotate=90]{experiment conditions (inputs)};
    \foreach \i in {0,...,4} \node (Ulabel\i) at (-8.5,5.6-2.66*\i) {$u_\i$};
  \end{tikzpicture}
  \caption{Treating the data as absolute, the fit changes ($u_0$ is
    the setup for the reference experiment). Some data points get an
    improved fit, some system behaviour cannot be reproduced. Note
    that this treatment of the data is incorrect and is done for
    illustration purposes; we show the difference between fitting
    absolute and relative data. To obtain these Trajectories, we have
    used a different, uninformative, iid, Gaussian prior during
    sampling. The resulting parameter sample is shown in
    Figure~\ref{fig:AbsoluteSample}}
  \label{fig:AbsoluteFit}
\end{figure}

\begin{figure}
  \centering 
  \input{GolgiSMMALAtikz}
  \caption{The parameters obtained from the assumption that provided
    data is absolute (molecule numbers or mols). Here, an
    uninformative prior was used.}
  \label{fig:AbsoluteSample}
\end{figure}

\subsection{Alternative sampling approach}

As in the given example, it is often the case that observations
$y_i(t_j,\theta,u_k)$ are reported in arbitrary units. However, we do
not fit the unknown data scaling constant. We expect this scaling to
be different when the experiment is repeated. In biological
experiments this unknown scaling will reflect conditions like dilution
of bacteria samples, exposure time for western blotting and similar,
which does change with repetition. Therefore, we are not interested in
the values of these scaling constants\footnote{see the mathematical
  documentation for details on this topic}. To deal with this issue,
we take the ratios between the observations and some reference
experiment results, which are often called <<\emph{control}>> by
biologists. This reference measurement is performed under identical
observation conditions and has the same scaling; thus, the scaling
cancels in the ratios. But, since the \emph{simulation} of this
reference value using the model depends on the parameters
$\rho=\exp(\theta)$, we have a parameter dependant scaling:
\begin{equation}
  \tilde y_{ijk}=\frac{y_i(t_j,\theta,u_k)}{y_i(t_j,\theta,u_0)}\,,
\end{equation}
where $u_0$ characterises the reference conditions. This results in
dramatically different fits. This method effectively eliminates all
observation parameters and results in a smaller MCMC problem than
trying to estimate the observational scaling.

Just for demonstration purposes, we ignore the source of the data and
pretend, that the measurements are absolute (gauged). The reference
experiment has been included as input line $u_0$. Although this is
wrong, the problem size is the same and the model can still fit some
of the data, but note that different data entries are fitted well
compared to the relative data fit. Figure~\ref{fig:AbsoluteFit} shows
this alternative fit. Specifically the model cannot fit the dynamic
behaviour in experiment $(u_3,y_4)$ and $(u_4,y_5)$ which it could
before, see Figure~\ref{fig:osmpl}. Most problematic is the fact that
the parameter fit cannot be reproduced by repeating the data
acquisition experiment; the scaling will be different.

\subsection{Model Setup}
\label{sec:model}
\lstinputlisting[caption={Model},label={xml_model}]{ODEmodel11S26P4U.xml}

\end{document}

