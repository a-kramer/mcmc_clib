\documentclass[english,12pt]{scrartcl}
\usepackage{lmodern}
\usepackage[osf]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{babel}
\usepackage{amsmath,amssymb}
\usepackage[bibstyle=numeric,backend=bibtex]{biblatex}
\usepackage{varioref}
\usepackage{array} % for tables with <{} and >{} column modifiers
\usepackage{booktabs}
\usepackage{capt-of}
\usepackage{multicol}
\usepackage{listingsutf8}
\lstset{language=bash,
        inputencoding=utf8,
        extendedchars=true,literate={«}{<<}1 {»}{>>}1,
        basicstyle=\ttfamily\small, 
        breaklines=true}
\usepackage[usenames,dvipsnames]{xcolor}
% some color definitions
\colorlet{lcolor}{blue!40!black}
\colorlet{ucolor}{magenta!40!black}
\colorlet{ccolor}{green!40!black}
\usepackage[colorlinks=true,%
            linkcolor=lcolor,%
            urlcolor=ucolor,%
            citecolor=ccolor]{hyperref} % makes references clickable
% style decisions
\setcapindent{0em}
\addtokomafont{captionlabel}{\bfseries}
\addtokomafont{title}{\rmfamily}
\addtokomafont{sectioning}{\rmfamily}
\frenchspacing


%bibliography
\addbibresource{NumericalSoftware.bib}


% makros
\newcommand{\CLIB}{\textsc{mcmc\_clib}}
\newcommand{\ode}{\textsc{ode}}
\newcommand{\noise}{\ensuremath{\varkappa}}

% titling
\subject{\texttt{ode\_smmala}}
\title{MCMC\_CLIB User Manual}
\author{Andrei Kramer}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
  This manual lists how to set up an \ode{} model for parameter
  sampling using the \CLIB{} software package. We have provided very
  general example files, which illustrate all uses of the software
  package. In the provided example the data is given in arbitrary
  units and only makes sense in relation to a reference
  experiment. Since the reference data also depends on the unknown
  parameters, the ratio has to be taken every time the model is used
  in the likelihood function. The example shows how to model unknown
  but stable initial conditions of perturbation experiments. This
  manual does not provide only the necessary documentation on the
  mathematical background.
\end{abstract}
\tableofcontents
\section{Setup}
\label{sec:setup}

This section provides all necessary commands to generate a shared
library which can be loaded by \CLIB{}. The initial value problem for
the given ODE model will be solved using CVODES. We use
\textsc{vfgen}~\cite{vfgen} for this purpose. Alongside the \ode{}
model, the user also supplies data, and information about how it was
obtained (sort of). Data often has to be normalised to some reference
signal. In many cases, a full quantification is too expensive, so two
(relative) signals are measured. They are meaningful in relation to
each other, but not on their own. In systems biology, this is the
typical case. We broadly distinguish between three cases: (a) a
control experiment was performed (system was in nominal state), (b) a
time series was obtained, where the relative changes between the
points are meaningful, and (c) a time series of several output signals
was obtained and normalisation happens between those signals at
specified time points. Relative data is often represented in the form
of percentages in publications (not in mol or other physical units). 

We consider the following system:
\begin{align}
  \dot x&=f(x,t;\rho,u)\,,& x(t_0;\rho,u)&=x_0\,,\label{eq:ode_model}\\
  y_{ijk}&\overset{!}{=} h_i(x(t_j;\rho,u_k) + \noise_{ijk}\,,& \noise &\sim \mathcal{N}(0,\sigma_{ijk})\,,
\end{align}
where, $\rho$ are the model parameters und $u$ are input parameters. A
dynamic input signal can be modeled within $f$ and is not made
explicit here. Notation: the semicolon is used to separate primary
arguments from parameters (contsant in one given simulation).
Currently, the initial conditions may not depend on the parameters or
inputs.

The output functions $h$ have access to one model simulation result,
while normalisation occurs between two simulations. Output functions
are specified in the model, normalisation is performed by \CLIB{}
internally. The user merely supplies the index structure of the
normalisation.

\subsection[make model]{Make Model\hfill\texttt{model.\{vf,xml\}}}
\label{sec:model}

\textsc{Vfgen} takes an \texttt{xml} file as input and exports into
many different formats including \textsc{cvodes} and
\textsc{matlab}. The extension \texttt{.vf} is arbitrary. Any other
tool that exports the model into a \textsc{cvodes} compatible header
and source file is of course just as good. \textsc{Vfgen} performs the
necessary analytical computations for the Jacobian and
parameter-Jacobian.

We use the xml-element \texttt{Parameter} to define model parameters
$\theta$ and inputs $u$. This list of parameters is ordered and the
\CLIB{} software will assume that the first $m$ appearing parameters
are the unknown sampling parameters $\theta$, where $m$ is defined
implicitely by setting a prior density in the \CLIB{} configuration
file, see Section~\vref{sec:configuration}. The remaining parameters
will be used as known input parameters. These inputs model the (known)
experimental conditions, they differ from one experiment to another.

To model output functions, we use the \texttt{Function} element, while
fluxes can be defined using the \texttt{Expression} element.

\subsection[make \textsc{cvodes} model sources]{Make \textsc{cvodes}
  model sources\hfill\texttt{model\_cvs.\{c,h\}}}
\label{sec:c}

Export the model:
\begin{verbatim}
$ vfgen cvodes:sens=yes,func=yes model.vf
\end{verbatim}

\subsection[make a shared library]{Make a Shared Library\hfill\texttt{model.so}}
\label{sec:so}

Compile the \texttt{vfgen} sources with \textsc{gcc}:
\begin{verbatim}
$ gcc -shared -fPIC -O2 -Wall -o model.so  model_cvs.c
\end{verbatim}

\subsection[write a configuration/data file]{Write a Configuration File\hfill\texttt{data.cfg}}
\label{sec:configuration}

The configuration file is a plain text file; it contains some
mandatory and some optional elements. Optional elements can be set for
convenience, the same elements can be set using command line
arguments. Command line arguments take precedence over the entries in
the configuration file.

The mandatory elements concern the data, the experimental conditions
under which the data was recorded and the prior knowledge about the
parameters. Most of the mandatory elements can be described as blocks
of numerical entries or matrices. This type of entry is done using
tags of the form \texttt{[entry\_type][/entry\_type]}:
\begin{description}
\item[\texttt{[time]}] A row of timepoints at which measurements have
  occured. This array has a meaning for all state variables. If some
  outputs were not observed at some time instance, this is noted in
  the \texttt{sd\_data} tag. The closing tag is \texttt{[/time]}.
\item[\texttt{[reference\_input]}] This block contains a row of known
  input parameters which define the model conditions corresponding to
  the reference experiment. If this block is present, then the
  experiment is considered relative. The closing tag is
  \texttt{[/reference\_input]} and similar for the following.
\item[\texttt{[input]}] contains one row per experiment, with columns
  corresponding to the input parameters $u$.
\item[\texttt{[reference\_data]}] block containing the measurements
  for the reference experiment; has one row per time point. columns
  represent output functions.
\item[\texttt{[data]}] The data block can take data from any number of
  performed experiments. Each experiment is associated with an input
  vector. Columns represent the output functions defined in the
  \texttt{vf} file. Rows represent measurement time points and
  experiment conditions (see Table~\ref{tab:datastrcuture}):
  measurements observed at timepoint $t_i$ ($i=0,\dots,T-1$) for input
  $u_j$ ($j=0,\dots,C-1$) will be in row $k=jT+i$
  ($k=0,\dots,CT-1$). For example, with $T$ time points and $N$
  experimental conditions, we get:
  \begin{multicols}{2}
  \begin{tabular}{>{$}c<{$} >{$}r<{$} >{$}l<{$}}
    \toprule
    \text{line}&\text{input row}&\text{timepoint}\\
    \midrule
    1&1& 1 \\
    2&1& 2 \\
    \vdots&1& \vdots \\
    &1& T\\
    &2& 1 \\
    &2& 2 \\
    &2& \vdots \\
    &2& T\\
    &\vdots&\vdots\\
    NT&N&T\\
    \bottomrule
  \end{tabular}
  \columnbreak
  \captionof{table}{Here we used line numbering starting at $1$; we get $CT$ lines
    for $C$ inputs (experiments) and $T$ timepoints at which
    measurements have occurred. Each line contains a row of data
    points. If a measurement was omitted you can enter any placeholder
    in the omitted slot and set the standard diviation of this point
    to \texttt{inf} ($\infty$).\label{tab:datastrcuture}}
  \end{multicols}
\item[\texttt{[sd\_data]}] This block is of the same size and
  structure as the data block and contains the standard deviations
  (uncertainties) of the data. Enter \texttt{inf} for any omitted
  measurement.
\item[\texttt{[prior\_mu]}] The prior is assumed to be log-normal on
  all unknown model parameters. Since sampling is done in logarithmic
  space, the prior is a multivariate normal $\mathcal{N}(\mu,\Sigma)$
  for the actual sampling variables. This block contains one column;
  it represents $\mu$ and has one row per unknown parameter. It is
  very important to set $\mu$ to values that make sense in
  \emph{logarithmic} space.
\item[\texttt{[prior\_inverse\_cov]}] This tag contains the $\Sigma$
  parameter of the Gaussian prior. It can also be written like this:
  \texttt{[prior\_inverse\_covariance]}.
\item[\texttt{[norm\_t]}] If you omit the \texttt{reference\_data} tag
  because you have an uncontrolled, relative time series, you can
  specify a time point for each experiment, where normalisation for
  all the measured outputs shall occur. In the simplest case, this is
  a column vector with one line per experiment, giving the index of
  normalisation time. Both \texttt{norm\_t} and \texttt{norm\_f} are explained in more detail in Section~\ref{sec:norm_ft}
\item[\texttt{[norm\_f]}] Sometimes, it makes sense to normalise the
  outputs between each other, within the same experiment. In that
  case, \texttt{norm\_t} must contain $C\times F$ elements (normalisation
  times), while \texttt{norm\_f} specifies by which output function
  index a given output function is to be normalised.
\end{description}
For convenience, you can set some of the command line options also as
variables inside the configuration file. Some variables can only be
set here. The follwong table lists all variables with example values:
\begin{center}
\begin{tabular}{r@{\texttt{=}}ll}
\toprule
variable name&value&meaning\\
\midrule
\texttt{step\_size}&\texttt{0.001}&Sampling algorithm's initial step size\\
\texttt{sample\_size}&\texttt{1000000}&\texttt{Integer}, sample size after warmup\\
\texttt{acceptance}&\texttt{0.50}&Target acceptance for tuning. \\
\texttt{output}&\texttt{./sample/model\_Y.double}&sample will be writen to this file.\\
\texttt{t0}&\texttt{-72}&$t_0$ for the initial conditions $x(t_0)=x_0$.\\
\bottomrule
\end{tabular}
\end{center}
Setting $t_0$ to a large negative value can be used to model
experiments which start in a stable but unknown equilibrium state of
the model. While the input function inside the model (\texttt{vf}
file) can be set to perform a perturbation at $t=0$.

\section{Time Series Normalisation}
\label{sec:norm_ft}

It is possible to specify a rather complex normalisation scheme using the elements \texttt{[norm\_f]} and \texttt{[norm\_t]}. Let us assume the following as the most general time series normalisation procedure
\begin{equation}
  \label{eq:output_norm_ft}
  \texttt{normalised} \tilde y_{i,j,b} := \frac{h_{i}(t_{j};\rho,u_b)}{h_{l(i,b)}(t_{k(i,b)};\rho,u_b)}=\frac{y_{ijb}}{y_{lkb}}\,,
\end{equation}
each output $y$ is normalised by dividing the output function $h$ at a
fixed time point $t_k$ of some other output function $h_l$, where $k$
and $l$ are different for each output channel $i$ and experiment $b$.

An example configuration could look like this:
\begin{lstlisting}
[norm_t]
2
[/norm_t]
\end{lstlisting}
This represents the following normalisation:
\begin{align}
  y_{ijk} \leftarrow \frac{y_{ijk}}{y_{i2k}}
\end{align}

If a different normalisation point is used for the different inputs, this notation applies:
\begin{lstlisting}
[norm_t]
2 # Experiment 0
0 # Experiment 1
2 # Experiment 2
5 # Experiment 3
[/norm_t]
\end{lstlisting}
This defines 4 experiments, where in experiment $0$, $y$ is normalised at $t_2$, while while $y_2$ is normalised at $t_0$, the first measurement instance.  

Finally, the most complex form of normalisation can be specified like this:
\begin{lstlisting}
[norm_f]
0 1 2 3 4 5 # This is ordered just like the data would be
1 1 1 1 4 5 # and specifies the data index to normalise with.
2 2 2 2 2 2 # Sometimes it is the same for all data-points.
4 4 4 4 4 4 # 4 experiments by 6 outputs
[/norm_f]

[norm_t]
2 2 2 2 2 2 # This is also ordered just like the data
3 3 3 3 2 2 # but specifies the time index to normalise at.
3 3 3 3 3 3 # Each measurement time t is normalised as any other 
3 3 3 3 3 3 # so this is just a C by F matrix again.
[/norm_t]
\end{lstlisting}
Here, we have $6$ output functions and $C=4$ experiments, no reference
experiment. The table lists the necessary $k$ and $l$ index for each output function:
\begin{align}
  \label{eq:norm_ft}
  \tilde y_{i,j,b} :&= \frac{y_{ijb}}{y_{lkb}}\,,\\
  \texttt{norm\_f}_{b,i} &= l(i,b)\,,\\
  \texttt{norm\_t}_{b,i} &= k(i,b)\,,
\end{align}
where $l$ is the output function we normalise with and $k$ is the time
index we normalise at. Each line represents a different experiment in
the data set.  We can infer the following information from the given
example:
\begin{center}
\begin{tabular}{rcl}
 $h_0(t;\rho,u_0)$ &is normalised by& $h_0(t_2;\rho,u_0)$\\
 $h_1(t;\rho,u_0)$ &is normalised by& $h_1(t_2;\rho,u_0)$\\
 $h_2(t;\rho,u_0)$ &is normalised by& $h_2(t_2;\rho,u_0)$\\
  \vdots&\vdots&\vdots\\
 $h_0(t;\rho,u_1)$ &is normalised by& $h_1(t_3;\rho,u_1)$\\
 $h_1(t;\rho,u_1)$ &is normalised by& $h_1(t_3;\rho,u_1)$\\
  \vdots&\vdots&\vdots\\
 $h_4(t;\rho,u_1)$ &is normalised by& $h_4(t_2;\rho,u_1)$\\
  \vdots&\vdots&\vdots\\
 $h_5(t;\rho,u_4)$ &is normalised by& $h_4(t_3;\rho,u_1)$\\
\end{tabular}
\end{center}
 


\section{Run Simulation}
\label{sim}

The program has some command line options, you can inspect the full list by requesting help: \texttt{./ode\_smmala -h}
\begin{verbatim}
~/mcmc_clib $ ./bin/ode_smmala -h
Usage:
-c ./data.cfg
                        data.cfg contains the data points and the conditions of measurement.

-l ./ode_model.so
                        ode_model.so is a shared library containing the CVODE functions of the model.

-s $N
                        $N sample size. default N=10.

-r, --resume
                        resume from last sampled MCMC point. Only the last MCMC position is read from the file named «resume.double». Everything else about the problem can be changed.
-o ./output_file
                        file for output. Output can be binary.

-b
                        output mode: binary.

-a $a
                        target acceptance value (markov chain will be tuned for this acceptance).

--seed $seed
                set the gsl pseudo random number generator seed to $seed.

\end{verbatim}
An example of how to run the program is included in the package:
\texttt{run\_test.sh}.
\lstinputlisting{./run_test.sh}

\section{Sample Analysis}
\label{sec:analysis}

The output will be logarithmic and will contain the log posterior for
each parameter vector. The sample can be read and processed using
numerical computation software like \textsc{GNU Octave}~\cite{octave:2012} or MATLAB:
\begin{verbatim}
fid=fopen('sample.double','r');
Sample=fread(fid,[NumberOfParameters+1,SampleSize],'double');
fclose(fid);
\end{verbatim}
To obtain model trajectories the parameter values have to be
transformed:
\begin{verbatim}
LogPosteriorIndex=NumberOfParameters+1;
[~,Imax]=max(Sample(LogPosteriorIndex,:));
MaxLikelihoodParas=exp(Sample(1:NumberOfParameters,Imax));
\end{verbatim}

\printbibliography
\end{document}
